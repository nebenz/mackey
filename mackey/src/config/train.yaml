defaults:
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

#  =========== changes variable ===============




check_val_every_n_epoch : 2
# cuda_visible_devices:  '0'

#  =========== data-sets ===============


ckpt_monitor : 'val_loss'
dataset_name: synthetic  


# database_folder : 'synthetic_data' 
data_set_path : /Users/Asaf/Documents/stocks_prophet/data_snp_500.pkl

# data_dir : 
#   train: ${data_set_path}/train.pickle  
#   val  : ${data_set_path}/val.pickle
#   test : ${data_set_path}/test.pickle



# =============== DEBUG ===============

debug_flag: False
debug: False
create_figs: False

# ========================================= test args ==============================================================








# =========== Model HP ==============

train_batch_size : 32 # 64
val_batch_size : 32 #64
test_batch_size :  32
pin_memory : False
data_loader_shuffle : True

window_size: 50 
prediction_step: 1
kernel_s: 5
input_ch: 1
hidden_size: 50
dilation_size: 1
layers: 5
drop_out: 0.1




# ======= Optimizer ===========

optimizer : Adam
learning_rate : 1e-4

# ======== Model  & Criterion ==========

model_type: dnn   
model_name: TCN #TCN_with_Residual_Encoder  # TCN # TCN_with_Residual # #  #   TCN,  TCN_with_Residual, TCN_with_Residual_Deepen, TCN_with_Residual_Encoder
criterion : MSELoss # L1Loss #MSELoss
#weights_loss: [4,0.6]
# criterion_paramter:  reduction  #None
criterion_paramter_value: none  #mean
# infer_from_ckpt: /workspace/inputs/asaf/mackey/models/model_TCN_loss_L1Loss_batch_32_optimizer_Adam_new/lightning_logs/version_0/checkpoints/epoch=659-val_loss=0.08.ckpt
# /workspace/inputs/asaf/mackey/models/model_TCN_loss_L1Loss_batch_32_optimizer_Adam_new/lightning_logs/version_0/checkpoints/last.ckpt
#  /workspace/inputs/asaf/mackey/models/model_TCN_loss_MSELoss_batch_64_optimizer_Adam_new/lightning_logs/version_0/checkpoints/epoch=57-val_loss=0.05.ckpt



# ========= ModelCheckPoints =========

patience : 10
save_top_k: 5
save_last: True
resume_from_checkpoint : None
precision : 16
progress_bar_refresh_rate : 5
log_gpu_memory : False
epsilon : 1e-8
num_workers : 30


# ================ CUDA ======================
gpus : -1 # -1
cuda_visible_devices:  '0,1,2,3,5,6,7,8'
# ========================================= Hydra config ==============================================================

model_def_name : model_${model_name}_loss_${criterion}_batch_${train_batch_size}_optimizer_${optimizer}_history_${window_size}_pat_25
models_dir :  /Users/Asaf/Dropbox/AI2C/Assignment/ml_assignment/exercise_4/mackey/models/${model_def_name}/ 


hydra:
  run:
    dir:  /Users/Asaf/Dropbox/AI2C/Assignment/ml_assignment/exercise_4/mackey/models/${hydra.job.override_dirname}/${hydra.job.name}/

  job:
    name: ${model_def_name}
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        # Remove params that would not impact the training itself
        # Remove all slurm and submit params.
        exclude_keys: [
          'hydra.job_logging.handles.file.filename',
          'dset.train', 'dset.valid', 'dset.test', 'dset.mix_json', 'dset.mix_dir',
          'num_prints', 'continue_from',
          'device', 'num_workers', 'print_freq', 'restart', 'verbose',
          'log', 'ddp', 'ddp_backend', 'rendezvous_file', 'rank', 'world_size', 'cuda_visible_devices']
  job_logging:
    handlers:
      file:
        class: logging.FileHandler
        mode: w
        formatter: colorlog
        filename: trainer.log
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr

  hydra_logging:
    handlers:
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr